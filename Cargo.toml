[package]
name = "scrapr"
version = "0.1.0"
edition = "2021"
authors = ["jrollin <jrollin@users.noreply.github.com>"]
license = "MIT"
description = "A fast, reliable Rust command-line tool for extracting structured content from web pages with intelligent tracking parameter cleanup"
homepage = "https://github.com/jrollin/scrapr"
repository = "https://github.com/jrollin/scrapr"
documentation = "https://docs.rs/scrapr"
readme = "README.md"
keywords = ["web-scraping", "cli", "metadata", "markdown", "url-cleaning"]
categories = ["command-line-utilities", "web-programming", "text-processing"]
exclude = [
    "target/*",
    ".git/*",
    ".github/*",
    "*.log",
    "Cargo.lock",
]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
reqwest = { version = "0.13.1", features = ["gzip"] }
# Json
serde = { version = "1.0.228", features = ["derive"] }
serde_json = "1.0.149"
# error
anyhow = "1.0.75"
thiserror = "2.0.17"
# parsing
webpage = { version = "2.0.1", default-features = false, features = ["serde"] }
clap = { version = "4.5.54", features = ["derive"] }
tokio = { version = "1.49.0", features = ["full"] }
# validation
url = "2.5.8"

[dev-dependencies]
mockito = "1.7.1"
